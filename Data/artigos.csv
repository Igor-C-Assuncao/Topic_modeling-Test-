ID,Title,Abstract,Keywords,Authors,Year,ISD,DOI,Links,Publication Type,Search Engine,VALIDAÇÃO,PDF
1535,Reconstructing Missing EHRs Using Time-Aware Within- and Cross-Visit Information for Septic Shock Early Prediction,"Real-world Electronic Health Records (EHRs) are often plagued by a high rate of missing data. In our EHRs, for example, the missing rates can be as high as 90% for some features, with an average missing rate of around 70% across all features. We propose a Time-Aware Dual-Cross-Visit missing value imputation method, named TA-DualCV, which spontaneously leverages multivariate dependencies across features and longitudinal dependencies both within- and cross-visit to maximize the information extracted from limited observable records in EHRs. Specifically, TA-DualCV captures the latent structure of missing patterns across measurements of different features and it also considers the time continuity and capture the latent temporal missing patterns based on both time-steps and irregular time-intervals. TA-DualCV is evaluated using three large real-world EHRs on two types of tasks: an unsupervised imputation task by varying mask rates up to 90% and a supervised 24-hour early prediction of septic shock using Long Short-Term Memory (LSTM). Our results show that TA-DualCV performs significantly better than all of the existing state-of-the-art imputation baselines, such as DETROIT and TAME, on both types of tasks.","Electronic Health Records(EHRs), EHRs Imputation, Septic Shock Early Prediction",G. Gao; F. Khoshnevisan; M. Chi,2022,2575-2634,10.1109/ICHI54592.2022.00034,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9874482,IEEE Conferences,IEEE,AC,2203.08245v2.pdf
16,3D-MICE: integration of cross-sectional and longitudinal imputation for multi-analyte longitudinal clinical data.,"A key challenge in clinical data mining is that most clinical datasets contain missing data. Since many commonly used machine learning algorithms require complete datasets (no missing data), clinical analytic approaches often entail an imputation procedure to ""fill in"" missing data. However, although most clinical datasets contain a temporal component, most commonly used imputation methods do not adequately accommodate longitudinal time-based data. We sought to develop a new imputation algorithm, 3-dimensional multiple imputation with chained equations (3D-MICE), that can perform accurate imputation of missing clinical time series data. We extracted clinical laboratory test results for 13 commonly measured analytes (clinical laboratory tests). We imputed missing test results for the 13 analytes using 3 imputation methods: multiple imputation with chained equations (MICE), Gaussian process (GP), and 3D-MICE. 3D-MICE utilizes both MICE and GP imputation to integrate cross-sectional and longitudinal information. To evaluate imputation method performance, we randomly masked selected test results and imputed these masked results alongside results missing from our original data. We compared predicted results to measured results for masked data points. 3D-MICE performed significantly better than MICE and GP-based imputation in a composite of all 13 analytes, predicting missing results with a normalized root-mean-square error of 0.342, compared to 0.373 for MICE alone and 0.358 for GP alone. 3D-MICE offers a novel and practical approach to imputing clinical laboratory time series data. 3D-MICE may provide an additional tool for use as a foundation in clinical predictive analytics and intelligent clinical decision support.","machine learning, imputation, missing data, electronic health record, EHR, multiple imputation with chained
equations, Gaussian process, computational pathology, data mining",Baron JM,2018,1527-974X,10.1093/jamia/ocx133,https://pubmed.ncbi.nlm.nih.gov/29202205/,"Journal Article Research Support, Non-U.S. Gov't",PubMed,AC,3D-MICE.pdf
1385,Performance evaluation of a recurrent deep neural network optimized by swarm intelligent techniques to model particulate matter.,"Atmospheric pollution refers to the presence of substances in the air such as particulate matter (PM) which has a negative impact in population ́s health exposed to it. This makes it a topic of current interest. Since the Metropolitan Zone of the Valley of Mexico's geographic characteristics do not allow proper ventilation and due to its population's density a significant quantity of poor air quality events are registered. This paper proposes a methodology to improve the forecasting of PM10 and PM2.5, in largely populated areas, using a recurrent long-term/short-term memory (LSTM) network optimized by the Ant Colony Optimization (ACO) algorithm. The experimental results show an improved performance in reducing the error by around 13.00% in RMSE and 14.82% in MAE using as reference the averaged results obtained by the LSTM deep neural network. Overall, the current study proposes a methodology to be studied in the future to improve different forecasting techniques in real-life applications where there is no need to respond in real time.<i>Implications</i>: This contribution presents a methodology to deal with the highly non-linear modeling of airborne particulate matter (both PM10 and PM2.5). Most linear approaches to this modeling problem are often not accurate enough when dealing with this type of data. In addition, most machine learning methods require extensive training or have problems when dealing with noise embedded in the time-series data. The proposed methodology deals with this data in three stages: preprocessing, modeling, and optimization. In the preprocessing stage, data is acquired and imputed any missing data. This ensures that the modeling process is robust even when there are errors in the acquired data and is invalid, or the data is missing. In the modeling stage, a recurrent deep neural network called LSTM (Long-Short Term Memory) is used, which shows that regardless of the monitoring station and the geographical characteristics of the site, the resulting model shows accurate and robust results. Furthermore, the optimization stage deals with enhancing the capability of the data modeling by using swarm intelligence algorithms (Ant Colony Optimization, in this case). The results presented in this study were compared with other works that presented traditional algorithms, such as multi-layer perceptron, traditional deep neural networks, and common spatiotemporal models, which show the feasibility of the methodology presented in this contribution. Lastly, the advantages of using this methodology are highlighted.",,Pedraza-Ortega JC,2022,2162-2906,10.1080/10962247.2022.2095057,https://pubmed.ncbi.nlm.nih.gov/35816429/,Journal Article,PubMed,AC,Performance evaluation of a recurrent deep neural network optimized by swarm intelligent techniques to model particulate matter.pdf
1433,Predicting progression of Alzheimer's disease using forward-to-backward bi-directional network with integrative imputation.,"If left untreated, Alzheimer's disease (AD) is a leading cause of slowly progressive dementia. Therefore, it is critical to detect AD to prevent its progression. In this study, we propose a bidirectional progressive recurrent network with imputation (BiPro) that uses longitudinal data, including patient demographics and biomarkers of magnetic resonance imaging (MRI), to forecast clinical diagnoses and phenotypic measurements at multiple timepoints. To compensate for missing observations in the longitudinal data, we use an imputation module to inspect both temporal and multivariate relations associated with the mean and forward relations inherent in the time series data. To encode the imputed information, we define a modification of the long short-term memory (LSTM) cell by using a progressive module to compute the progression score of each biomarker between the given timepoint and the baseline through a negative exponential function. These features are used for the prediction task. The proposed system is an end-to-end deep recurrent network that can accomplish multiple tasks at the same time, including (1) imputing missing values, (2) forecasting phenotypic measurements, and (3) predicting the clinical status of a patient based on longitudinal data. We experimented on 1,335 participants from The Alzheimer's Disease Prediction of Longitudinal Evolution (TADPOLE) challenge cohort. The proposed method achieved a mean area under the receiver-operating characteristic curve (mAUC) of 78% for predicting the clinical status of patients, a mean absolute error (MAE) of 3.5ml for forecasting MRI biomarkers, and an MAE of 6.9ml for missing value imputation. The results confirm that our proposed model outperforms prevalent approaches, and can be used to minimize the progression of Alzheimer's disease.","Alzheimer’s progression, MRI biomarker forecasting, Missing value imputation, Clinical status prediction, Progressive recurrent networks",Pant S,2022,1879-2782,S0893-6080(22)00094-6,https://pubmed.ncbi.nlm.nih.gov/35364417/,Journal Article,PubMed,AC,1-s2.0-S0893608022000946-main.pdf
369,Causes and Consequences of Missing Health-Related Quality of Life Assessments in Patients Who Undergo Mechanical Circulatory Support Implantation: Insights From INTERMACS (Interagency Registry for Mechanically Assisted Circulatory Support).,"Missing health-related quality of life (HRQOL) data in longitudinal studies can reduce precision and power and bias results. Using INTERMACS (Interagency Registry for Mechanically Assisted Circulatory Support), we sought to identify factors associated with missing HRQOL data, examine the impact of these factors on estimated HRQOL assuming missing at random missingness, and perform sensitivity analyses to examine missing not at random (MNAR) missingness because of illness severity. INTERMACS patients (n=3248) with a preimplantation profile of 1 (critical cardiogenic shock) or 2 (progressive decline) were assessed with the EQ-5D-3L visual analog scale and Kansas City Cardiomyopathy Questionnaire-12 summary scores pre-implantation and 3 months postoperatively. Mean and median observed and missing at random-imputed HRQOL scores were calculated, followed by sensitivity analyses. Independent factors associated with HRQOL scores and missing HRQOL assessments were determined using multivariable regression. Independent factors associated with preimplantation and 3-month HRQOL scores, and with the likelihood of missing HRQOL assessments, revealed few correlates of HRQOL and missing assessments (<i>R</i><sup>2</sup> range, 4.7%-11.9%). For patients with INTERMACS profiles 1 and 2 and INTERMACS profile 1 alone, missing at random-imputed mean and median HRQOL scores were similar to observed scores, before and 3 months after implantation, whereas MNAR-imputed mean scores were lower (≥5 points) at baseline but not at 3 months. We recommend use of sensitivity analyses using an MNAR imputation strategy for longitudinal studies when missingness is attributable to illness severity. Conduct of MNAR sensitivity analyses may be less critical after mechanical circulatory support implant, when there are likely fewer MNAR data.","longitudinal studies , quality of life , registry , shock, cardiogenic , visual analog scale",Spertus JA,2017,1941-7705,10.1161/CIRCOUTCOMES.116.003268,https://pubmed.ncbi.nlm.nih.gov/29246883/,Comparative Study Journal Article Multicenter Study,PubMed,AC,grady2017.pdf
375,CGCNImp: a causal graph convolutional network for multivariate time series imputation.,"Multivariate time series data generally contains missing values, which can be an obstacle to subsequent analysis and may compromise downstream applications. One challenge in this endeavor is the presence of the missing values brought about by sensor failure and transmission packet loss. Imputation is the usual remedy in such circumstances. However, in some multivariate time series data, the complex correlation and temporal dependencies, coupled with the non-stationarity of the data, make imputation difficult. To address this problem, we propose a novel model for multivariate time series imputation called CGCNImp that considers both correlation and temporal dependency modeling. The correlation dependency module leverages neural Granger causality and a GCN to capture the correlation dependencies among different attributes of the time series data, while the temporal dependency module relies on an attention-driven long short term memory (LSTM) and a time lag matrix to learn its dependencies. Missing values and noise are addressed with total variation reconstruction. We conduct thorough empirical analyses on two real-world datasets. Imputation results show that CGCNImp achieves state-of-the-art performance when compared to previous methods.","Multivariate time series imputation, Graph causal analysis, Graph neural network,
Deep neural network",Liu S,2022,2376-5992,10.7717/peerj-cs.966,https://pubmed.ncbi.nlm.nih.gov/35634128/,Journal Article,PubMed,AC,peerj-cs-966.pdf
447,Concurrent Imputation and Prediction on EHR Data Using Bi-Directional GANs: Bi-GANs for EHR Imputation and Prediction,"Working with electronic health records (EHRs) is known to be challenging due to several reasons. These reasons include not having: 1) similar lengths (per visit), 2) the same number of observations (per patient), and 3) complete entries in the available records. These issues hinder the performance of the predictive models created using EHRs. In this paper, we approach these issues by presenting a model for the combined task of imputing and predicting values for the irregularly observed and varying length EHR data with missing entries. Our proposed model (dubbed as Bi-GAN) uses a bidirectional recurrent network in a generative adversarial setting. In this architecture, the generator is a bidirectional recurrent network that receives the EHR data and imputes the existing missing values. The discriminator attempts to discriminate between the actual and the imputed values generated by the generator. Using the input data in its entirety, Bi-GAN learns how to impute missing elements in-between (imputation) or outside of the input time steps (prediction). Our method has three advantages to the state-of-the-art methods in the field: (a) one single model performs both the imputation and prediction tasks; (b) the model can perform predictions using time-series of varying length with missing data; (c) it does not require to know the observation and prediction time window during training and can be used for the predictions with different observation and prediction window lengths, for short- and long-term predictions. We evaluate our model on two large EHR datasets to impute and predict body mass index (BMI) values and show its superior performance in both settings.","Recurrent Neural Network, Adversarial Training, Electronic Health
Record","Gupta M,Phan TL,Bunnell HT,Beheshti R",2021,  ,10.1145/3459930.3469512,https://doi.org/10.1145/3459930.3469512,Conference Paper,ACM,AC,gupta2021.pdf
1656,Septic Shock Prediction for Patients with Missing Data,"Sepsis and septic shock are common and potentially fatal conditions that often occur in intensive care unit (ICU) patients. Early prediction of patients at risk for septic shock is therefore crucial to minimizing the effects of these complications. Potential indications for septic shock risk span a wide range of measurements, including physiological data gathered at different temporal resolutions and gene expression levels, leading to a nontrivial prediction problem. Previous works on septic shock prediction have used small, carefully curated datasets or clinical measurements that may not be available for many ICU patients. The recent availability of a large, rich ICU dataset called MIMIC-II has provided the opportunity for more extensive modeling of this problem. However, such a large clinical dataset inevitably contains a substantial amount of missing data. We investigate how different imputation selection criteria and methods can overcome the missing data problem. Our results show that imputation methods in conjunction with predictive modeling can lead to accurate septic shock prediction, even if the features are restricted primarily to noninvasive measurements. Our models provide a generalized approach for predicting septic shock in any ICU patient.","Missing data, imputation methods, sepsis, data mining","Ho JC,Lee CH,Ghosh J",2014,2158-656X,10.1145/2591676,https://doi.org/10.1145/2591676,Journal Article,ACM,AC,ho2014.pdf
718,Evaluating the state of the art in missing data imputation for clinical data.,"Clinical data are increasingly being mined to derive new medical knowledge with a goal of enabling greater diagnostic precision, better-personalized therapeutic regimens, improved clinical outcomes and more efficient utilization of health-care resources. However, clinical data are often only available at irregular intervals that vary between patients and type of data, with entries often being unmeasured or unknown. As a result, missing data often represent one of the major impediments to optimal knowledge derivation from clinical data. The Data Analytics Challenge on Missing data Imputation (DACMI) presented a shared clinical dataset with ground truth for evaluating and advancing the state of the art in imputing missing data for clinical time series. We extracted 13 commonly measured blood laboratory tests. To evaluate the imputation performance, we randomly removed one recorded result per laboratory test per patient admission and used them as the ground truth. DACMI is the first shared-task challenge on clinical time series imputation to our best knowledge. The challenge attracted 12 international teams spanning three continents across multiple industries and academia. The evaluation outcome suggests that competitive machine learning and statistical models (e.g. LightGBM, MICE and XGBoost) coupled with carefully engineered temporal and cross-sectional features can achieve strong imputation performance. However, care needs to be taken to prevent overblown model complexity. The challenge participating systems collectively experimented with a wide range of machine learning and probabilistic algorithms to combine temporal imputation and cross-sectional imputation, and their design principles will inform future efforts to better model clinical missing data.","missing data imputation, machine learning, clinical laboratory test, time series",Luo Y,2022,1477-4054,10.1093/bib/bbab489,https://pubmed.ncbi.nlm.nih.gov/34882223/,"Journal Article Research Support, N.I.H., Extramural",PubMed,AC,bbab489.pdf
1282,Multivariate Time Series Imputation With Transformers,"Processing time series with missing segments is a fundamental challenge that puts obstacles to advanced analysis in various disciplines such as engineering, medicine, and economics. One of the remedies is imputation to fill the missing values based on observed values properly without undermining performance. We propose the Multivariate Time-Series Imputation with Transformers (MTSIT), a novel method that uses transformer architecture in an unsupervised manner for missing value imputation. Unlike the existing transformer architectures, this model only uses the encoder part of the transformer due to computational benefits. Crucially, MTSIT trains the autoencoder by jointly reconstructing and imputing stochastically-masked inputs via an objective designed for multivariate time-series data. The trained autoencoder is then evaluated for imputing both simulated and real missing values. Experiments show that MTSIT outperforms state-of-the-art imputation methods over benchmark datasets.","Deep learning, imputation, multivariate time series, time series, transformer, unsupervised learning.",A. Y. Yıldız; E. Koç; A. Koç,2022,1558-2361,10.1109/LSP.2022.3224880,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9964035,IEEE Journals,IEEE,AC,Multivariate_Time_Series_Imputation_With_Transformers.pdf
816,"Gender equality policies, nursing professionalization, and the nursing workforce: A cross-sectional, time-series analysis of 22 countries, 2000-2015.","Nursing professionalization has substantial benefits for patients, health care systems, and the nursing workforce. Currently, however, there is limited understanding of the macro-level factors, such as policies and other country-level determinants, influencing both the professionalization process and the supply of nursing human resources. Given the significance of gender to the development of nursing, a majority-female occupation, the purpose of this analysis was to investigate the relationship between gender regimes and gender equality policies, as macro-level determinants, and nursing professionalization indicators, in this case the regulated nurse and nurse graduate ratios. This cross-sectional, time-series analysis covered 16 years, from 2000 to 2015, and included 22 high-income countries, members of the Organisation for Economic Co-operation and Development. We divided countries into three clusters, using the gender policy model developed by Korpi, as proxy for gender regimes. The countries were grouped as follows: (a) Traditional family - Austria, Belgium, France, Germany, Greece, Italy, Netherlands, Portugal, and Spain; (b) Market-oriented - Australia, Canada, Ireland, Japan, New Zealand, South Korea, Switzerland, United Kingdom, and the United States; and (c) Earner-carer - Denmark, Finland, Norway, and Sweden. We used fixed-effects linear regression models and ran Prais-Winsten regressions with panel-corrected standard errors, including a first-order autocorrelation correction to examine the effect of gender equality policies on nursing professionalization indicators. Given the existence of missing observations, we devised and implemented a multiple imputation strategy, with the help of the Amelia II program. We gathered our data from open access secondary sources. Both the regulated nurse and nurse graduate ratios had averages that differed across gender regimes, being the highest in Earner-carer regimes and the lowest in Traditional family ones. In addition, we identified a number of indicators of gender equality policy in education, the labour market, and politics that are predictive of the regulated nurse and nurse graduate ratios. This study's findings could add to existing upstream advocacy efforts to strengthen nursing and the nursing workforce through healthy public policy. Given that the study consists of an international comparative analysis of nursing, it should be relevant to both national and global nursing communities.","Family policies,Fixed-effects linear regression, Gender equality policies, Gender regimes,  Health equity, Health human resources, Nurses/midwives/nursing, Nursing professionalization, Patient and health system outcomes, Politics of health and health care",Chung H,2019,1873-491X,S0020-7489(19)30187-7,https://pubmed.ncbi.nlm.nih.gov/31493758/,Historical Article Journal Article,PubMed,AC,gunn2019.pdf
855,Handling Missing Values in Longitudinal Panel Data With Multiple Imputation.,"This article offers an applied review of key issues and methods for the analysis of longitudinal panel data in the presence of missing values. The authors consider the unique challenges associated with attrition (survey dropout), incomplete repeated measures, and unknown observations of time. Using simulated data based on 4 waves of the Marital Instability Over the Life Course Study (n = 2,034), they applied a fixed effect regression model and an event-history analysis with time-varying covariates. They then compared results for analyses with nonimputed missing data and with imputed data both in long and in wide structures. Imputation produced improved estimates in the event-history analysis but only modest improvements in the estimates and standard errors of the fixed effects analysis. Factors responsible for differences in the value of imputation are examined, and recommendations for handling missing values in panel data are presented.","event history analysis, fixed effects, longitudinal, data, missing data, multiple imputation, panel data.",Johnson DR,2015,0022-2445,10.1111/jomf.12144,https://pubmed.ncbi.nlm.nih.gov/26113748/,Journal Article,PubMed,AC,young2015.pdf
935,Imputation of missing time-activity data with long-term gaps: A multi-scale residual CNN-LSTM network model.,"Despite the increasing availability and spatial granularity of individuals' time-activity (TA) data, the missing data problem, particularly long-term gaps, remains as a major limitation of TA data as a primary source of human mobility studies. In the present study, we propose a two-step imputation method to address the missing TA data with long-term gaps, based on both efficient representation of TA patterns and high regularity in TA data. The method consists of two steps: (1) the continuous bag-of-words word2vec model to convert daily TA sequences into a low-dimensional numerical representation to reduce complexity; (2) a multi-scale residual Convolutional Neural Network (CNN)-stacked Long Short-Term Memory (LSTM) model to capture multi-scale temporal dependencies across historical observations and to predict the missing TAs. We evaluated the performance of the proposed imputation method using the mobile phone-based TA data collected from 180 individuals in western New York, USA, from October 2016 to May 2017, with a 10-fold out-of-sample cross-validation method. We found that the proposed imputation method achieved excellent performance with 84% prediction accuracy, which led us to conclude that the proposed imputation method was successful at reconstructing the sequence, duration, and spatial extent of activities from incomplete TA data. We believe that the proposed imputation method can be applied to impute incomplete TA data with relatively long-term gaps with high accuracy.","Missing data, Long-term gaps in time-activity data, Imputation, Embedding, Multi-scale residual CNN-stacked LSTM",Yoo EH,2022,0198-9715,10.1016/j.compenvurbsys.2022.101823,https://pubmed.ncbi.nlm.nih.gov/35812524/,Journal Article,PubMed,AC,1-s2.0-S0198971522000679-am.pdf
1723,Spatiotemporal Imputation of MAIAC AOD Using Deep Learning with Downscaling.,"Aerosols have adverse health effects and play a significant role in the climate as well. The Multiangle Implementation of Atmospheric Correction (MAIAC) provides Aerosol Optical Depth (AOD) at high temporal (daily) and spatial (1 km) resolution, making it particularly useful to infer and characterize spatiotemporal variability of aerosols at a fine spatial scale for exposure assessment and health studies. However, clouds and conditions of high surface reflectance result in a significant proportion of missing MAIAC AOD. To fill these gaps, we present an imputation approach using deep learning with downscaling. Using a baseline autoencoder, we leverage residual connections in deep neural networks to boost learning and parameter sharing to reduce overfitting, and conduct bagging to reduce error variance in the imputations. Downscaled through a similar auto-encoder based deep residual network, Modern-Era Retrospective analysis for Research and Applications Version 2 (MERRA-2) GMI Replay Simulation (M2GMI) data were introduced to the network as an important gap-filling feature that varies in space to be used for missingness imputations. Imputing weekly MAIAC AOD from 2000 to 2016 over California, a state with considerable geographic heterogeneity, our full (non-full) residual network achieved mean R<sup>2</sup> = 0.94 (0.86) [RMSE = 0.007 (0.01)] in an independent test, showing considerably better performance than a regular neural network or non-linear generalized additive model (mean R<sup>2</sup> = 0.78-0.81; mean RMSE = 0.013-0.015). The adjusted imputed as well as combined imputed and observed MAIAC AOD showed strong correlation with Aerosol Robotic Network (AERONET) AOD (R = 0.83; R<sup>2</sup> = 0.69, RMSE = 0.04). Our results show that we can generate reliable imputations of missing AOD through a deep learning approach, having important downstream air quality modeling applications.","Aerosol Optical Depth, MAIAC, MERRA-2 GMI  Replay Simulation, Deep learning, Downscaling, Missingness imputation, Air quality",Habre R,2020,0034-4257,10.1016/j.rse.2019.111584,https://pubmed.ncbi.nlm.nih.gov/32158056/,Journal Article,PubMed,AC,10.1016@j.rse.2019.111584.pdf
1785,Temporal Graph Signal Decomposition,"Temporal graph signals are multivariate time series with individual components associated with nodes of a fixed graph structure. Data of this kind arises in many domains including activity of social network users, sensor network readings over time, and time course gene expression within the interaction network of a model organism. Traditional matrix decomposition methods applied to such data fall short of exploiting structural regularities encoded in the underlying graph and also in the temporal patterns of the signal. How can we take into account such structure to obtain a succinct and interpretable representation of temporal graph signals?We propose a general, dictionary-based framework for temporal graph signal decomposition (TGSD). The key idea is to learn a low-rank, joint encoding of the data via a combination of graph and time dictionaries. We propose a highly scalable decomposition algorithm for both complete and incomplete data, and demonstrate its advantage for matrix decomposition, imputation of missing values, temporal interpolation, clustering, period estimation, and rank estimation in synthetic and real-world data ranging from traffic patterns to social media activity. Our framework achieves 28% reduction in RMSE compared to baselines for temporal interpolation when as many as 75% of the observations are missing. It scales best among baselines taking under 20 seconds on 3.5 million data points and produces the most parsimonious models. To the best of our knowledge, TGSD is the first framework to jointly model graph signals by temporal and graph dictionaries.","graph mining, signal processing, time series, dictionary coding. sparse decomposition, interpolation, periodicity detection","McNeil MJ,Zhang L,Bogdanov P",2021,  ,10.1145/3447548.3467379,https://dl.acm.org/doi/10.1145/3447548.3467379,Conference Paper,ACM,AC,3447548.3467379.pdf
1228,Multi-matrices factorization with application to missing sensor data imputation.,"We formulate a multi-matrices factorization model (MMF) for the missing sensor data estimation problem. The estimation problem is adequately transformed into a matrix completion one. With MMF, an n-by-t real matrix, R, is adopted to represent the data collected by mobile sensors from n areas at the time, T1, T2, ..., Tt, where the entry, Rij, is the aggregate value of the data collected in the ith area at Tj. We propose to approximate R by seeking a family of d-by-n probabilistic spatial feature matrices, U(1), U(2), ..., U(t), and a probabilistic temporal feature matrix, [formula in text]. We also present a solution algorithm to the proposed model. We evaluate MMF with synthetic data and a real-world sensor dataset extensively. Experimental results demonstrate that our approach outperforms the state-of-the-art comparison algorithms.","matrix factorization; sensor data; probabilistic graphical model;
missing estimation",Cai WX,2013,1424-8220,10.3390/s131115172,https://pubmed.ncbi.nlm.nih.gov/24201318/,"Journal Article Research Support, Non-U.S. Gov't",PubMed,AC,sensors-13-15172-v2.pdf
2009,Weighted Gate Layer Autoencoders.,"A single dataset could hide a significant number of relationships among its feature set. Learning these relationships simultaneously avoids the time complexity associated with running the learning algorithm for every possible relationship, and affords the learner with an ability to recover missing data and substitute erroneous ones by using available data. In our previous research, we introduced the gate-layer autoencoders (GLAEs), which offer an architecture that enables a single model to approximate multiple relationships simultaneously. GLAE controls what an autoencoder learns in a time series by switching on and off certain input gates, thus, allowing and disallowing the data to flow through the network to increase network's robustness. However, GLAE is limited to binary gates. In this article, we generalize the architecture to weighted gate layer autoencoders (WGLAE) through the addition of a weight layer to update the error according to which variables are more critical and to encourage the network to learn these variables. This new weight layer can also be used as an output gate and uses additional control parameters to afford the network with abilities to represent different models that can learn through gating the inputs. We compare the architecture against similar architectures in the literature and demonstrate that the proposed architecture produces more robust autoencoders with the ability to reconstruct both incomplete synthetic and real data with high accuracy.","Autoencoder, Neural Networks, Unsupervised, Learning, Data Reconstruction.",Abbass HA,2022,2168-2275,10.1109/TCYB.2021.3049583,https://pubmed.ncbi.nlm.nih.gov/33502995/,Journal Article,PubMed,AC,CYB_E_2020_04_0849_final_GLAE_Manuscript.pdf